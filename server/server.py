import argparse
import datetime
import fnmatch
import json
import logging
import os
import sqlite3
import sys
import tempfile
import threading
import uuid

import yaml
from markitdown import MarkItDown
from tornado import gen, ioloop, web, websocket
from tornado.httpclient import AsyncHTTPClient

if os.name == "nt":
    import msvcrt
else:
    import fcntl

# è°ƒè¯•æ¨¡å¼é…ç½®
DEBUG = os.getenv("DEBUG", "false").lower() == "true"
CACHE_DEFAULT_SECONDS = 60
# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    stream=sys.stdout,
    force=True,
)
logger = logging.getLogger(__name__)

connected_clients = {}
pending_requests = {}
FILTER_KEY = "filters"
main_config = {FILTER_KEY: []}
config_file_path = os.path.join(os.path.dirname(__file__), "config.yaml")


def load_config():
    """åŠ è½½å¹¶ç¼–è¯‘é€‰æ‹©å™¨é…ç½®"""
    try:
        with open(config_file_path, "r") as f:
            main_config.update(yaml.safe_load(f))
            logger.info("âœ… æˆåŠŸåŠ è½½ %d æ¡selectoré…ç½®", len(main_config["filters"]))
    except Exception as e:
        logger.error("ğŸš¨ åŠ è½½selectors.yamlå¤±è´¥: %s", str(e))


def save_config():
    """å°†å½“å‰é…ç½®å†™å…¥é…ç½®æ–‡ä»¶"""
    try:
        # å†™å…¥é…ç½®æ–‡ä»¶
        with open(config_file_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(main_config, f, allow_unicode=True)
        logger.info("ğŸ’¾ é…ç½®å·²æˆåŠŸå†™å…¥ç£ç›˜")
    except Exception as e:
        logger.error("ğŸš¨ å†™å…¥é…ç½®æ–‡ä»¶å¤±è´¥: %s", str(e))


def init_cache_db():
    """åˆå§‹åŒ–SQLiteç¼“å­˜æ•°æ®åº“"""
    with sqlite3.connect("url_cache.db") as conn:
        cursor = conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS url_cache (
                url TEXT PRIMARY KEY,
                markdown_content TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        """
        )
        conn.commit()
        logger.info("âœ… åˆå§‹åŒ–URLç¼“å­˜æ•°æ®åº“å®Œæˆ")


# è·¨å¹³å°æ–‡ä»¶é”
class ProcessLock:
    def __init__(self, lock_file="server.lock"):
        logger.info("ğŸ”’ åˆå§‹åŒ–è¿›ç¨‹é”ï¼Œæ–‡ä»¶: %s", lock_file)
        self.lock_file = lock_file
        self.locking = threading.Lock()
        self.fd = None
        self.file = None

    def acquire(self):
        logger.info("ğŸ” å°è¯•è·å–è¿›ç¨‹é”")
        try:
            self.file = open(self.lock_file, "w", encoding="utf-8")
            self.fd = self.file.fileno()
            if os.name == "nt":  # Windows
                logger.info("ğŸªŸ æ£€æµ‹åˆ°Windowsç³»ç»Ÿï¼Œä½¿ç”¨msvcrté”å®š")
                try:
                    msvcrt.locking(self.fd.fileno(), msvcrt.LK_NBLCK, 1)
                    logger.info("âœ… æˆåŠŸè·å–Windowsè¿›ç¨‹é”")
                    return True
                except IOError:
                    logger.warning("âš ï¸ Windowsè¿›ç¨‹é”å·²è¢«å ç”¨")
                    return False
            else:  # Unix/Linux/Mac
                logger.info("ğŸ§ æ£€æµ‹åˆ°Unix/Linux/Macç³»ç»Ÿï¼Œä½¿ç”¨fcntlé”å®š")
                try:
                    fcntl.flock(self.fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                    logger.info("âœ… æˆåŠŸè·å–Unixè¿›ç¨‹é”")
                    return True
                except (IOError, BlockingIOError):
                    logger.warning("âš ï¸ Unixè¿›ç¨‹é”å·²è¢«å ç”¨")
                    return False
        except (OSError, IOError) as e:
            logger.error("ğŸš¨ è·å–é”å¤±è´¥: %s", str(e))
            return False

    def release(self):
        with self.locking:
            logger.info("ğŸ”“ å°è¯•é‡Šæ”¾è¿›ç¨‹é”")
            try:
                if self.fd:
                    if os.name == "nt":
                        logger.info("ğŸªŸ é‡Šæ”¾Windowsè¿›ç¨‹é”")
                        msvcrt.locking(self.fd.fileno(), msvcrt.LK_UNLCK, 1)
                    else:
                        logger.info("ğŸ§ é‡Šæ”¾Unixè¿›ç¨‹é”")
                        fcntl.flock(self.fd, fcntl.LOCK_UN)
                    self.file.close()
                    os.unlink(self.lock_file)
                    self.fd = None
                    logger.info("âœ… æˆåŠŸé‡Šæ”¾è¿›ç¨‹é”")
            except (OSError, IOError) as e:
                logger.error("ğŸš¨ é‡Šæ”¾é”å¤±è´¥: %s", str(e))


class BrowserWebSocketHandler(websocket.WebSocketHandler):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, *kwargs)
        self.client_id = None
        logger.info("ğŸ›  åˆå§‹åŒ–WebSocketå¤„ç†å™¨")

    def check_origin(self, origin):
        logger.info("ğŸŒ æ£€æŸ¥æ¥æº: %s", origin)
        return (
            origin.startswith("chrome-extension://")
            or origin.startswith("http://localhost:")
            or origin.startswith("http://127.0.0.1:")
        )

    def open(self, *args, **kwargs):
        self.client_id = str(uuid.uuid4())
        connected_clients[self.client_id] = self
        logger.info("ğŸ® æµè§ˆå™¨å®¢æˆ·ç«¯è¿æ¥æˆåŠŸï¼ŒID: %s", self.client_id)
        logger.info("ğŸ“Š å½“å‰è¿æ¥å®¢æˆ·ç«¯æ•°: %d", len(connected_clients))

    def on_message(self, message):
        logger.info("ğŸ“¨ æ”¶åˆ°æµè§ˆå™¨æ¶ˆæ¯: %s...", message[:200])
        ioloop.IOLoop.current().add_callback(self._process_message, message)

    async def _process_message(self, message):
        try:
            logger.debug("ğŸ“¨ åŸå§‹æ¶ˆæ¯: %s", message)
            data = json.loads(message)
            logger.debug("ğŸ“ è§£æåæ•°æ®: %s", data)
            logger.info("ğŸ“ è§£ææ¶ˆæ¯ç±»å‹: %s", data.get("type"))

            if data.get("type") == "htmlResponse":
                logger.debug("ğŸ”„ å¤„ç†htmlResponseæ¶ˆæ¯")
                await self._handle_html_response(data)
            elif data.get("type") == "selectorConfig":
                logger.debug("ğŸ”„ å¤„ç†selectorConfigæ¶ˆæ¯")
                await self._handle_selector_config(data)

        except (json.JSONDecodeError, KeyError) as e:
            logger.error("ğŸš¨ å¤„ç†æ¶ˆæ¯å‡ºé”™: %s", str(e), exc_info=True)

    async def _handle_html_response(self, data):
        request_id = data.get("requestId")
        content = data.get("content", "")
        logger.info("âœ… è¯·æ±‚ %s ç»“æœå·²æ”¶åˆ°, å†…å®¹é•¿åº¦: %d", request_id, len(content))
        if request_id in pending_requests:
            logger.debug("ğŸ“¦ æ‰¾åˆ°pending_requestsä¸­çš„è¯·æ±‚: %s", request_id)
            pending_requests[request_id].set_result(content)
            logger.info("âœ… è¯·æ±‚ %s å·²è®¾ç½®ç»“æœ", request_id)
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°pending_requestsä¸­çš„è¯·æ±‚: %s", request_id)

    async def _handle_selector_config(self, data):
        logger.debug("âš™ï¸ æ”¶åˆ°selectoré…ç½®æ•°æ®: %s", data)
        url = data.get("url")
        selector = data.get("selector")
        logger.debug("ğŸ”— URL: %s, é€‰æ‹©å™¨: %s", url, selector)

        if not url or not selector:
            logger.warning("âš ï¸ æ— æ•ˆçš„selectoré…ç½®: ç¼ºå°‘urlæˆ–selector")
            return

        existing_config = self._find_existing_config(url)
        logger.debug("ğŸ” æŸ¥æ‰¾ç°æœ‰é…ç½®ç»“æœ: %s", existing_config)

        if existing_config:
            logger.debug("ğŸ”„ æ›´æ–°ç°æœ‰é…ç½®")
            self._update_existing_config(existing_config, selector, url)
        else:
            logger.debug("ğŸ†• æ·»åŠ æ–°é…ç½®")
            self._add_new_config(url, selector)

        save_config()
        logger.debug("ğŸ’¾ é…ç½®å·²ä¿å­˜")

    def _find_existing_config(self, url):
        logger.debug("ğŸ” åœ¨main_configä¸­æŸ¥æ‰¾URL: %s", url)
        result = next((item for item in main_config[FILTER_KEY] if item["pattern"] == url), None)
        logger.debug("ğŸ” æŸ¥æ‰¾ç»“æœ: %s", result)
        return result

    def _update_existing_config(self, existing_config, selector, url):
        existing_config["pattern"] = url
        existing_config["selectors"] = [selector]
        logger.info("ğŸ”„ æ›´æ–°ç°æœ‰selectoré…ç½®: %s -> %s", url, selector)

    def _add_new_config(self, url, selector):
        new_config = {"pattern": url, "selectors": [selector]}
        logger.debug("ğŸ†• æ·»åŠ æ–°é…ç½®: %s", new_config)
        main_config[FILTER_KEY].append(new_config)
        logger.info("âœ… æ·»åŠ æ–°selectoré…ç½®: %s -> %s", url, selector)
        logger.debug("ğŸ†• æ·»åŠ åmain_config: %s", main_config[FILTER_KEY])

    def data_received(self, chunk):
        pass

    def on_close(self):
        if self.client_id in connected_clients:
            del connected_clients[self.client_id]
        logger.info("âŒ æµè§ˆå™¨å®¢æˆ·ç«¯æ–­å¼€ï¼ŒID: %s", self.client_id)
        logger.info("ğŸ“Š å½“å‰è¿æ¥å®¢æˆ·ç«¯æ•°: %d", len(connected_clients))


class ConvertHandler(web.RequestHandler):
    def data_received(self, chunk):
        pass

    async def _process_html(self, html, is_news):
        if is_news:
            logger.info("ğŸ›  æ­£åœ¨ä½¿ç”¨Readabilityå‡€åŒ–å†…å®¹...")
            try:
                http_client = AsyncHTTPClient()
                logger.info("ğŸŒ å‘ReadabilityæœåŠ¡å‘é€è¯·æ±‚")
                response = await http_client.fetch(
                    "http://localhost:3000/html_reader",
                    method="POST",
                    headers={"Content-Type": "application/json"},
                    body=json.dumps({"content": html}),
                    connect_timeout=10,
                    request_timeout=30,
                )
                if response.code == 200:
                    result = json.loads(response.body)
                    if "content" in result:
                        html = result["content"]
                        logger.info("âœ… å‡€åŒ–å®Œæˆï¼Œæ–°é•¿åº¦: %s å­—ç¬¦", len(html))
                    else:
                        logger.warning("âš ï¸ å‡€åŒ–æœåŠ¡æœªè¿”å›æœ‰æ•ˆå†…å®¹ï¼Œä½¿ç”¨åŸå§‹HTML")
                else:
                    logger.error("âš ï¸ å‡€åŒ–æœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ç : %s", response.code)
            except (OSError, IOError, ValueError) as e:
                logger.error("ğŸš¨ å‡€åŒ–æœåŠ¡è°ƒç”¨å¤±è´¥: %sï¼Œç»§ç»­ä½¿ç”¨åŸå§‹HTML", str(e))
        return html

    async def _convert_to_markdown(self, html):
        try:
            with tempfile.NamedTemporaryFile(mode="w", suffix=".html", delete=True, encoding="utf-8") as f:
                f.write(html)
                f.flush()
                logger.info("ğŸ”„ å¼€å§‹è½¬æ¢ï¼Œä¸´æ—¶æ–‡ä»¶: %s", f.name)
                md = MarkItDown()
                result = md.convert(f.name)
                logger.info("âœ… è½¬æ¢å®Œæˆï¼ŒMarkdowné•¿åº¦: %s å­—ç¬¦", len(result.text_content))
                return result.text_content
        except (OSError, IOError):
            logger.warning("âš ï¸ æ— æ³•åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œå°è¯•æ™®é€šæ–‡ä»¶")
            temp_file = "temp_conversion.html"
            try:
                with open(temp_file, "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info("ğŸ”„ å¼€å§‹è½¬æ¢ï¼Œä¸´æ—¶æ–‡ä»¶: %s", temp_file)
                md = MarkItDown()
                result = md.convert(temp_file)
                logger.info("âœ… è½¬æ¢å®Œæˆï¼ŒMarkdowné•¿åº¦: %s å­—ç¬¦", len(result.text_content))
                return result.text_content
            finally:
                try:
                    os.remove(temp_file)
                except OSError:
                    logger.warning("âš ï¸ æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶: %s", temp_file)

    async def get(self):
        try:
            url = self.get_query_argument("url")
            is_news = self.get_query_argument("is_news", "false").lower() == "true"
            logger.info("ğŸŒ æ”¶åˆ°è½¬æ¢è¯·æ±‚ï¼ŒURL: %s", url)

            # æ–°å¢é€‰æ‹©å™¨åŒ¹é…é€»è¾‘
            matched_selectors = []
            cache_seconds = CACHE_DEFAULT_SECONDS  # é»˜è®¤ç¼“å­˜æ—¶é—´
            for entry in main_config[FILTER_KEY]:
                if fnmatch.fnmatch(url, entry["pattern"]):  # ä½¿ç”¨globæ¨¡å¼åŒ¹é…URL
                    matched_selectors = entry["selectors"]
                    cache_seconds = entry.get("cache_seconds", CACHE_DEFAULT_SECONDS)  # è·å–é…ç½®çš„ç¼“å­˜æ—¶é—´
                    logger.info(
                        "ğŸ” URLåŒ¹é…åˆ°glob: %s, ç¼“å­˜æ—¶é—´: %dç§’",
                        entry["pattern"],
                        cache_seconds,
                    )
                    break

            # æ£€æŸ¥ç¼“å­˜
            try:
                with sqlite3.connect("url_cache.db") as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT markdown_content, created_at FROM url_cache WHERE url = ?",
                        (url,),
                    )
                    if row := cursor.fetchone():
                        content, created_at = row
                        # è®¡ç®—ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                        created_time = datetime.datetime.fromisoformat(created_at)
                        time_diff = (datetime.datetime.now() - created_time).total_seconds()
                        if time_diff <= cache_seconds:
                            logger.info("ğŸ’¾ å‘½ä¸­æœ‰æ•ˆç¼“å­˜ï¼Œç›´æ¥è¿”å›ç»“æœ")
                            return self.write(content)
                        logger.info(
                            "â³ ç¼“å­˜å·²è¿‡æœŸï¼Œæ—¶é—´å·®: %.1fç§’ > %dç§’",
                            time_diff,
                            cache_seconds,
                        )
            except sqlite3.Error as e:
                logger.error("ğŸš¨ ç¼“å­˜æŸ¥è¯¢å¤±è´¥: %s", str(e))

            if not connected_clients:
                logger.error("ğŸš« æ²¡æœ‰è¿æ¥çš„æµè§ˆå™¨å®¢æˆ·ç«¯")
                self.set_status(503)
                return self.write({"error": "No browser connected"})

            client = next(iter(connected_clients.values()))
            request_id = str(uuid.uuid4())
            fut = gen.Future()
            pending_requests[request_id] = fut
            logger.info("ğŸ†” ç”Ÿæˆè¯·æ±‚ID: %s", request_id)

            try:
                logger.info("ğŸ“¤ æ­£åœ¨è½¬å‘æå–è¯·æ±‚åˆ°æµè§ˆå™¨: (URL: %s, ID: %s)", url, request_id)
                # æ·»åŠ è¿æ¥çŠ¶æ€æ£€æŸ¥
                if client.ws_connection is None or client.ws_connection.is_closing():
                    raise web.HTTPError(503, reason="WebSocket connection closed")
                await client.write_message(
                    json.dumps(
                        {
                            "type": "extract",
                            "url": url,
                            "requestId": request_id,
                            "selectors": matched_selectors,  # æ–°å¢é€‰æ‹©å™¨å­—æ®µ
                        }
                    )
                )

                html = await gen.with_timeout(ioloop.IOLoop.current().time() + 60, fut)
                logger.info("ğŸ“¥ æ”¶åˆ°HTMLå“åº”ï¼Œé•¿åº¦: %s å­—ç¬¦", len(html))

                html = await self._process_html(html, is_news)
                markdown = await self._convert_to_markdown(html)

                # å†™å…¥ç¼“å­˜
                try:
                    with sqlite3.connect("url_cache.db") as conn:
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            INSERT OR REPLACE INTO url_cache
                            (url, markdown_content, created_at)
                            VALUES (?, ?, ?)
                        """,
                            (url, markdown, datetime.datetime.now().isoformat()),
                        )
                        conn.commit()
                        logger.info("ğŸ’¾ ç¼“å­˜å†™å…¥æˆåŠŸ")
                except sqlite3.Error as e:
                    logger.error("ğŸš¨ ç¼“å­˜å†™å…¥å¤±è´¥: %s", str(e))

                self.write(markdown)

            except gen.TimeoutError:
                logger.error("â° è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ±‚ID: %s", request_id)
                self.set_status(504)
                self.write({"error": "Request timeout"})
            except websocket.WebSocketClosedError:
                logger.error("WebSocketè¿æ¥å·²å…³é—­")
                self.set_status(503)
                self.write({"error": "WebSocket connection closed"})
            finally:
                pending_requests.pop(request_id, None)
        except web.MissingArgumentError:
            self.set_status(400)
            self.write({"error": "Missing url parameter"})

        except (OSError, IOError, ValueError) as e:
            logger.error("å¤„ç†è¯·æ±‚å‡ºé”™: %s", str(e))
            self.set_status(500)
            self.write({"error": "Internal server error"})


def make_app():
    return web.Application(
        [
            (r"/convert", ConvertHandler),
            (r"/ws", BrowserWebSocketHandler),
        ]
    )


if __name__ == "__main__":
    # æ·»åŠ å‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="å¯åŠ¨æœåŠ¡å™¨ã€‚")
    parser.add_argument("--addr", default="127.0.0.1", help="æœåŠ¡å™¨ç›‘å¬åœ°å€ (é»˜è®¤: 127.0.0.1)")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡å™¨ç›‘å¬ç«¯å£ (é»˜è®¤: 8000)")
    parsed_args = parser.parse_args()

    # åˆ›å»ºè¿›ç¨‹é”
    process_lock = ProcessLock()
    if not process_lock.acquire():
        logger.error("ğŸš« å·²æœ‰æœåŠ¡å™¨å®ä¾‹åœ¨è¿è¡Œï¼Œè¯·å…ˆåœæ­¢å½“å‰å®ä¾‹")
        sys.exit(1)

    try:
        load_config()
        init_cache_db()  # åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“
        app = make_app()
        # ä½¿ç”¨å‚æ•°ä¸­çš„åœ°å€å’Œç«¯å£
        app.listen(parsed_args.port, address=parsed_args.addr)
        logger.info("%s", f"ğŸš€ æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ {parsed_args.addr}:{parsed_args.port}")

        ioloop.IOLoop.current().start()
    finally:
        process_lock.release()
