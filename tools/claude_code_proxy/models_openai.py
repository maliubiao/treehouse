from __future__ import annotations

from typing import Any, Dict, List, Literal, NamedTuple, Optional

from pydantic import BaseModel, Field

# --- Request Models ---


class OpenAIFunction(BaseModel):
    """Model for the function definition within a tool."""

    name: str
    description: str
    parameters: Dict[str, Any]  # JSON Schema


class OpenAITool(BaseModel):
    """Model for a single tool provided to the model."""

    type: Literal["function"] = "function"
    function: OpenAIFunction


class OpenAIFunctionCall(BaseModel):
    """Model for the function details within a tool call."""

    name: str
    arguments: str  # JSON string


class OpenAIToolCall(BaseModel):
    """Model for a tool call generated by the OpenAI model."""

    id: str
    type: Optional[Literal["function"]] = "function"
    function: OpenAIFunctionCall


class OpenAIChatMessage(BaseModel):
    """Model for a single message in an OpenAI Chat Completions request."""

    role: Literal["system", "user", "assistant", "tool"]
    content: Optional[str] = Field(default=None)
    tool_calls: Optional[List[OpenAIToolCall]] = Field(default=None)
    tool_call_id: Optional[str] = Field(default=None)
    name: Optional[str] = Field(default=None)  # For tool role, this can be the function name


class OpenAIRequest(BaseModel):
    """Model for a request to the OpenAI Chat Completions API."""

    model: str
    messages: List[OpenAIChatMessage]
    stream: Optional[bool] = Field(default=False)
    temperature: Optional[float] = Field(default=None)
    max_tokens: Optional[int] = Field(default=None)
    tools: Optional[List[OpenAITool]] = Field(default=None)


# --- Response Models ---


# Non-streaming response models
class OpenAIResponseMessage(BaseModel):
    content: Optional[str] = None
    tool_calls: Optional[List[OpenAIToolCall]] = None
    role: Literal["assistant"]
    reasoning_content: Optional[str] = None


class OpenAIChoice(BaseModel):
    finish_reason: Optional[str] = None
    index: int
    message: OpenAIResponseMessage


class OpenAIUsage(BaseModel):
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int


class OpenAIChatCompletion(BaseModel):
    id: str
    choices: List[OpenAIChoice]
    created: int
    model: str
    object: Literal["chat.completion"]
    usage: OpenAIUsage


class OpenAIChunkUsage(BaseModel):
    """
    Optional usage statistics for a chat completion chunk.
    Fields are optional as they may only appear in the final chunk of a stream.
    """

    completion_tokens: Optional[int] = Field(default=None)
    prompt_tokens: Optional[int] = Field(default=None)
    total_tokens: Optional[int] = Field(default=None)


class OpenAIFunctionCallDelta(BaseModel):
    arguments: Optional[str] = None
    name: Optional[str] = None


class OpenAIToolCallDelta(BaseModel):
    index: int
    id: Optional[str] = None
    type: Optional[Literal["function"]] = None
    function: Optional[OpenAIFunctionCallDelta] = None


class OpenAIChatMessageDelta(BaseModel):
    content: Optional[str] = None
    role: Optional[Literal["assistant"]] = None
    tool_calls: Optional[List[OpenAIToolCallDelta]] = None
    reasoning_content: Optional[str] = None


class OpenAIChoiceDelta(BaseModel):
    delta: OpenAIChatMessageDelta
    finish_reason: Optional[str] = None
    index: int


class OpenAIChatCompletionChunk(BaseModel):
    id: str
    choices: List[OpenAIChoiceDelta]
    created: int
    model: str
    object: Literal["chat.completion.chunk"]
    usage: Optional[OpenAIChunkUsage] = Field(default=None)


# 定义一个简单的命名元组来模拟 vLLM 的 ExtractedToolCallInformation
class ExtractedToolCallInfo(NamedTuple):
    tools_called: bool
    tool_calls: List[OpenAIToolCall]
    content: Optional[str]
